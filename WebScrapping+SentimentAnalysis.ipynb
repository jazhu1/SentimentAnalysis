{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbZAnayFnRtJ"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "!pip install praw\n",
        "import requests\n",
        "import pandas as pd\n",
        "import praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my details\n",
        "reddit = praw.Reddit(client_id='X1xh8N9D99a34s9BoRYCFA', client_secret='0KvruvvglL3-iUbf6T6Vtv9zyuWZFQ', user_agent='Scrapping - 301')"
      ],
      "metadata": {
        "id": "lxtD13_DserQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting reddit post\n",
        "post_url = 'https://www.reddit.com/r/technology/comments/123nfrx/theres_a_90_chance_tiktok_will_be_banned_in_the/'\n",
        "post = reddit.submission(url=post_url)\n",
        "comments = post.comments.list()\n"
      ],
      "metadata": {
        "id": "XcqqPXfqsltf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seeing the number of comments\n",
        "post.num_comments"
      ],
      "metadata": {
        "id": "_1W3K3LTthze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98370f72-8c3d-4792-e955-e57a23957c95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5527"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#package for comments within post\n",
        "from praw.models import MoreComments\n",
        "\n",
        "#creating empty list for post comments\n",
        "post_comments = []\n",
        "\n",
        "#collecting the body of each comment in the comment forest\n",
        "for comment in post.comments:\n",
        "\tif type(comment) == MoreComments:\n",
        "\t\tcontinue\n",
        "\n",
        "#appends the comment body\n",
        "\tpost_comments.append(comment.body)\n",
        "\n",
        "#create dataframe and putting all comments under \"comments\" column\n",
        "all_comments_df = pd.DataFrame(post_comments, columns=['comments'])\n",
        "all_comments_df\n"
      ],
      "metadata": {
        "id": "Y5eFISfbuhF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#including all the comments within comments\n",
        "all_comments = []\n",
        "\n",
        "#including all the comments within \"load more comments\"\n",
        "post.comments.replace_more(limit=None)\n",
        "#replace the MoreComments objects with actual Comment objects\n",
        "#iterates over all the comments in the comment forest\n",
        "for comment in post.comments.list():\n",
        " all_comments.append([comment.body, comment.id, comment.score, comment.created])\n",
        "# creating a dataframe\n",
        "all_comments_df = pd.DataFrame(all_comments, columns=['comment', 'ID', 'score', 'created'])\n",
        "all_comments_df\n"
      ],
      "metadata": {
        "id": "uGBHl7mBuxc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at our dataframe\n",
        "all_comments_df.shape"
      ],
      "metadata": {
        "id": "QFK1dk8xzTFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the created date\n",
        "\n",
        "from datetime import datetime\n",
        "#all time details obtained\n",
        "all_comments_df['date_time'] = pd.to_datetime(all_comments_df['created'],  unit='s')\n",
        "#create a string with only the date\n",
        "all_comments_df['date'] = all_comments_df['date_time'].dt.strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "id": "b_G8AmFnzHWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at our dataframe\n",
        "all_comments_df.shape"
      ],
      "metadata": {
        "id": "A7CIwLIXbaQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the duplicates\n",
        "cleaned_comments_df = all_comments_df\n",
        "cleaned_comments_df.drop_duplicates(subset='comment', keep='last', inplace=True)\n",
        "cleaned_comments_df.to_csv('post_comments.csv', index=True, header=True)\n",
        "#compare the new data after dropping duplicates\n",
        "cleaned_comments_df.shape"
      ],
      "metadata": {
        "id": "4BqYmcEXzLjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view columns\n",
        "cleaned_comments_df.columns"
      ],
      "metadata": {
        "id": "xYM-eSbXbDR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install package for text cleaning\n",
        "!pip install spacy\n",
        "import spacy\n",
        "#pretrained model containing word features and more\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "#process text and extract linguistic features\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "VWp1qNZQfAeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I will use re.sub for substituting string with a replacement string\n",
        "import re\n",
        "#remove the stopwords from text for NLP\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "0SbtRgh8bmBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function designed to clean the text\n",
        "def reddit_clean(redd):\n",
        "    redd = str(redd).lower()\n",
        "    redd = re.sub(\"'\", \"\", redd)\n",
        "    redd = re.sub(\"@[A-Za-z0-9_]+\",\"\", redd)\n",
        "    redd = re.sub(\"#[A-Za-z0-9_]+\",\"\", redd)\n",
        "    redd = re.sub(r\"www.\\S+\", \"\", redd)\n",
        "    redd = re.sub(r\"http\\S+\", \"\", redd)\n",
        "    redd = re.sub('[()!?]', ' ', redd)\n",
        "    redd = re.sub('\\[.*?\\]',' ', redd)\n",
        "    redd = re.sub(\"[^a-z0-9]\",\" \", redd)\n",
        "    redd = re.sub(r\"\\b\\w{1,3}\\b\",\" \", redd)\n",
        "    redd = redd.split()\n",
        "    #nltk stopwords to be removed\n",
        "    stopwords = STOPWORDS\n",
        "    redd = [w for w in redd if not w in stopwords]\n",
        "    doc = nlp(\" \".join(redd))\n",
        "    #using spacy to remove parts of speech that are not needed\n",
        "    redd = [token.text for token in doc if token.pos_ not in ['PRON', 'ADV','SCONJ', 'DET', 'AUX','ADP', 'PART']]\n",
        "    redd = \" \".join(word for word in redd)\n",
        "    return redd\n",
        "\n",
        "cleaned_comments_df['comment'] = cleaned_comments_df['comment'].apply(reddit_clean)\n",
        "cleaned_comments_df.head()"
      ],
      "metadata": {
        "id": "uTFt3PDnhVW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the comments\n",
        "tokenized_reddit_post = cleaned_comments_df['comment'].apply(lambda x: x.split())\n",
        "print(tokenized_reddit_post)"
      ],
      "metadata": {
        "id": "xL4QZBmIb2AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stem the words\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tokenized_reddit_post = tokenized_reddit_post.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "cleaned_comments_df['tokenized']= tokenized_reddit_post\n",
        "cleaned_comments_df.head()"
      ],
      "metadata": {
        "id": "hq4Ht-F9cAN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary packages for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import collections\n",
        "#form a single string of all the words in the comments\n",
        "all_words = ' '.join([text for text in cleaned_comments_df['comment']])\n",
        "#dictionary where each key is a word and the corresponding value is the frequency of that word in the comments\n",
        "count_word = collections.Counter(all_words.split())\n",
        "\n",
        "#creating word cloud\n",
        "wordcloud = WordCloud(width=800, height=500)\n",
        "wordcloud.generate_from_frequencies(count_word)\n",
        "plt.figure(figsize=(10, 7)) # In inches\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c_P4i0_VcKbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import collections\n",
        "#count the most words shown up in all_words\n",
        "count_word = collections.Counter(all_words.split())\n",
        "count_word.most_common(15)"
      ],
      "metadata": {
        "id": "2emib3UHcWE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bar chart for top 10 frequent words\n",
        "import seaborn as sns\n",
        "a = nltk.FreqDist(count_word)\n",
        "d = pd.DataFrame({'Reddit': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "#selecting top 10 most frequent words\n",
        "d = d.nlargest(columns=\"Count\", n = 10)\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Reddit\", y = \"Count\")\n",
        "ax.set(ylabel = 'Word Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJhe_QyXcX73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using vader for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "#initialize VADER analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#create new columns for sentiment scores\n",
        "cleaned_comments_df['neg'] = cleaned_comments_df['comment'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n",
        "cleaned_comments_df['neu'] = cleaned_comments_df['comment'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n",
        "cleaned_comments_df['pos'] = cleaned_comments_df['comment'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n",
        "cleaned_comments_df['compound'] = cleaned_comments_df['comment'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "\n",
        "cleaned_comments_df.head()"
      ],
      "metadata": {
        "id": "uAra00ZhpjmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using texblob for sentiment analysis (subjectivity)\n",
        "from textblob import TextBlob\n",
        "\n",
        "#obtaining subjectivity of the words\n",
        "cleaned_comments_df['subjectivity'] = cleaned_comments_df['comment'].apply(lambda y: TextBlob(y).sentiment.subjectivity)\n",
        "cleaned_comments_df.head()"
      ],
      "metadata": {
        "id": "a8AiTMEt20wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a function to classify the sentiment using vader\n",
        "def analyze_sentiment(text):\n",
        "    sentiment_scores = analyzer.polarity_scores(text)\n",
        "    compound_score = sentiment_scores['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        return 'Positive Sentiment'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'Negative Sentiment'\n",
        "    else:\n",
        "        return 'Neutral Sentiment'"
      ],
      "metadata": {
        "id": "nMRzGXmDpx1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NdF9BJgp3ewP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create new column for sentiment and polarity using vader\n",
        "cleaned_comments_df['sentiment'] = cleaned_comments_df['comment'].apply(lambda x: analyze_sentiment(x))\n",
        "\n",
        "cleaned_comments_df['polarity'] = cleaned_comments_df['comment'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "\n",
        "#create a new subjectivity column\n",
        "cleaned_comments_df['subjectivity2'] = np.where(cleaned_comments_df['subjectivity'] > 0.5, 'Subjective', 'Objective')\n",
        "cleaned_comments_df.head()"
      ],
      "metadata": {
        "id": "2RNB3lZC3W_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "#select only the rows with Negative Sentiment\n",
        "neg_comments = cleaned_comments_df[cleaned_comments_df['sentiment'] == 'Negative Sentiment']\n",
        "\n",
        "#join all the comments into a single string\n",
        "neg_text = ' '.join(neg_comments['comment'].tolist())\n",
        "\n",
        "# split the string into words\n",
        "neg_words = neg_text.split()\n",
        "\n",
        "#count the frequency of each word\n",
        "neg_word_counts = Counter(neg_words)\n",
        "\n",
        "#get the top 10 most frequent words\n",
        "top_neg_words = neg_word_counts.most_common(10)\n",
        "words = [word[0] for word in top_neg_words]\n",
        "counts = [count[1] for count in top_neg_words]\n",
        "\n",
        "#creating bar chart\n",
        "plt.bar(words, counts)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Use of Top 10 Words within Negative Sentiment Context')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "te69NowUR_t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select only the rows with Negative Sentiment\n",
        "pos_comments = cleaned_comments_df[cleaned_comments_df['sentiment'] == 'Positive Sentiment']\n",
        "\n",
        "#join all the comments into a single string\n",
        "pos_text = ' '.join(pos_comments['comment'].tolist())\n",
        "\n",
        "#split the string into words\n",
        "pos_words = pos_text.split()\n",
        "\n",
        "#count the frequency of each word\n",
        "pos_word_counts = Counter(pos_words)\n",
        "\n",
        "#get the top 10 most frequent words\n",
        "top_pos_words = pos_word_counts.most_common(10)\n",
        "words = [word[0] for word in top_pos_words]\n",
        "counts = [count[1] for count in top_pos_words]\n",
        "\n",
        "#barchart\n",
        "plt.bar(words, counts)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Use of Top 10 Words within Positive Sentiment Context')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yjb2-DQmTq0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select only the rows with Negative Sentiment\n",
        "neu_comments = cleaned_comments_df[cleaned_comments_df['sentiment'] == 'Neutral Sentiment']\n",
        "\n",
        "#join all the comments into a single string\n",
        "neu_text = ' '.join(neu_comments['comment'].tolist())\n",
        "\n",
        "#split the string into words\n",
        "neu_words = neu_text.split()\n",
        "\n",
        "#count the frequency of each word\n",
        "neu_word_counts = Counter(neu_words)\n",
        "\n",
        "#get the top 10 most frequent words\n",
        "top_neu_words = neu_word_counts.most_common(10)\n",
        "words = [word[0] for word in top_neu_words]\n",
        "counts = [count[1] for count in top_neu_words]\n",
        "\n",
        "#barchart\n",
        "plt.bar(words, counts)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Use of Top 10 Words within Neutral Sentiment Context')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arza1MwMT-Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bar chart of the sentiment\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.xlabel('Sentiment', fontsize=30)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.ylabel('Frequency', fontsize=30)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.hist(cleaned_comments_df['polarity'], bins=25)\n",
        "plt.title('Sentiment Distribution', fontsize=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "97Zb_ak03kCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bar chart for subjectivity\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.xlabel('Subjectivity', fontsize=30)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.ylabel('Frequency', fontsize=30)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.hist(cleaned_comments_df['subjectivity'], bins=15)\n",
        "plt.title('Subjectivity Distribution', fontsize=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N_9n4bGI5Ev8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#descriptive statistics for polarity and subjectivity\n",
        "pol_sub = cleaned_comments_df[[\"polarity\", \"subjectivity\"]]\n",
        "pol_sub.describe()"
      ],
      "metadata": {
        "id": "bulKNfh25KPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#table for the subjectivity of comments\n",
        "pd.crosstab(cleaned_comments_df['sentiment'], cleaned_comments_df['subjectivity2'])"
      ],
      "metadata": {
        "id": "slA7y-1o5SfM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}